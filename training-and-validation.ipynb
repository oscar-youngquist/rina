{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basis-Function Meta-Learaning for Rapid Introspective Neural Adaptation (RINA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script loads quadrotor flight data in different wind conditions, trains a wind invariant representation of the unmodeled aerodynamics, and tests the performance of the model when adapting to new data in different wind conditions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "import os \n",
    "import utils\n",
    "import mlmodel\n",
    "\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.platform == 'win32':\n",
    "    NUM_WORKERS = 0 # Windows does not support multiprocessing\n",
    "else:\n",
    "    NUM_WORKERS = 2\n",
    "print('running on ' + sys.platform + ', setting ' + str(NUM_WORKERS) + ' workers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data and create some simple visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_a = 3\n",
    "# features = ['q', 'q_dot', 'tau']\n",
    "# label = 'tau_residual_m'\n",
    "\n",
    "features = ['q', 'q_dot', 'tau_cmd']\n",
    "label = 'tau_residual_cmd'\n",
    "\n",
    "labels = [\"FR_hip\", \"FR_knee\", \"FR_foot\", \"FL_hip\", \"FL_knee\", \"FL_foot\",\n",
    "          \"RR_hip\", \"RR_knee\", \"RR_foot\", \"RL_hip\", \"RL_knee\", \"RL_foot\"]\n",
    "\n",
    "# Training data collected from the neural-fly drone\n",
    "dataset = 'rina' \n",
    "# dataset_folder = '/home/hcr/Research/DARoSLab/DARoS-Core/lcm_converted_log/training_data/'\n",
    "# dataset_folder = '/home/hcr/Research/DARoSLab/DARoS-Core/lcm_converted_log/03_04_2024/unitree_estimate/training_data/'\n",
    "# dataset_folder = '/home/hcr/Research/DARoSLab/DARoS-Core/lcm_converted_log/05_17_2024_formal/training_data/'\n",
    "# testdata_folder = \"/home/hcr/Research/DARoSLab/DARoS-Core/lcm_converted_log/05_17_2024_formal/eval_data/\"\n",
    "\n",
    "\n",
    "dataset_folder = '/home/oyoungquist/Research/RINA/rina/data/lcm_converted_log/05_17_2024_formal/training_data/'\n",
    "testdata_folder = \"/home/oyoungquist/Research/RINA/rina/data/lcm_converted_log/05_17_2024_formal/eval_data/\"\n",
    "\n",
    "\n",
    "\n",
    "now = datetime.now() # current date and time\n",
    "date_time = now.strftime(\"%m/%d/%Y%H:%M:%S\")\n",
    "date_time = re.sub('[^0-9a-zA-Z]+', '_', date_time)\n",
    "\n",
    "date_time += \"_cmd_residual\"\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "output_path_base = os.path.join(cwd, \"training_results\", date_time)\n",
    "\n",
    "if not os.path.exists(output_path_base):\n",
    "    os.makedirs(output_path_base)\n",
    "\n",
    "print(output_path_base)\n",
    "\n",
    "# # Training data collected from an intel aero drone\n",
    "# dataset = 'neural-fly-transfer'\n",
    "# dataset_folder = 'data/training-transfer'\n",
    "# hover_pwm = 910 # mean hover pwm for neural-fly drone\n",
    "# intel_hover_pwm = 1675 # mean hover pwm for intel-aero drone\n",
    "# hover_pwm_ratio = hover_pwm / intel_hover_pwm # scaling ratio from system id\n",
    "\n",
    "modelname = f\"{dataset}_dim-a-{dim_a}_{'-'.join(features)}\" # 'intel-aero_fa-num-Tsp_v-q-pwm'\n",
    "\n",
    "print(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RawData = utils.load_data(dataset_folder)\n",
    "Data = utils.format_data(RawData, features=features, output=label, body_offset=0)\n",
    "\n",
    "print(\"\\n-----------------------------------------------\\n\")\n",
    "\n",
    "RawData = utils.load_data(testdata_folder) # expnames='(baseline_)([0-9]*|no)wind'\n",
    "TestData = utils.format_data(RawData, features=features, output=label, body_offset=0) # wind condition label, C, will not make sense for this data - that's okay since C is only used in the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_images = os.path.join(output_path_base, \"training_data_images\")\n",
    "test_data_images = os.path.join(output_path_base, \"test_data_images\")\n",
    "\n",
    "if not os.path.exists(training_data_images):\n",
    "    os.makedirs(training_data_images)\n",
    "\n",
    "if not os.path.exists(test_data_images):\n",
    "    os.makedirs(test_data_images)\n",
    "\n",
    "for data in Data:\n",
    "    utils.plot_subdataset(data, features, labels, os.path.join(training_data_images, \"{:s}.png\".format(data.meta['condition'])), title_prefix=\"(Training data)\")\n",
    "\n",
    "for data in TestData:\n",
    "    utils.plot_subdataset(data, features, labels, os.path.join(test_data_images, \"{:s}.png\".format(data.meta['condition'])), title_prefix=\"(Testing Data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize some other hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {}\n",
    "options['dim_x'] = Data[0].X.shape[1]\n",
    "options['dim_y'] = Data[0].Y.shape[1]\n",
    "options['num_c'] = len(Data)\n",
    "print('dims of (x, y) are', (options['dim_x'], options['dim_y']))\n",
    "print('there are ' + str(options['num_c']) + ' different conditions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "options['features'] = features\n",
    "options['dim_a'] = dim_a\n",
    "options['loss_type'] = 'crossentropy-loss'\n",
    "\n",
    "options['shuffle'] = True # True: shuffle trajectories to data points\n",
    "options['K_shot'] = 32 # number of K-shot for least square on a\n",
    "options['phi_shot'] = 256 # batch size for training phi\n",
    "\n",
    "options['alpha'] = 0.01 # adversarial regularization loss\n",
    "options['learning_rate'] = 5e-4\n",
    "options['frequency_h'] = 2 # how many times phi is updated between h updates, on average\n",
    "options['SN'] = 2. # maximum single layer spectral norm of phi\n",
    "options['gamma'] = 10. # max 2-norm of a\n",
    "options['num_epochs'] = 2000\n",
    "\n",
    "\n",
    "options['output_path'] = output_path_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adaptation dataset will be used to update $a$ in each training loop.\n",
    "The training dataset will be used to train $\\phi$ in each training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainset = []\n",
    "# Adaptset = []\n",
    "Trainloader = []\n",
    "Adaptloader = []\n",
    "for i in range(options['num_c']):\n",
    "    fullset = mlmodel.MyDataset(Data[i].X, Data[i].Y, Data[i].C)\n",
    "    \n",
    "    l = len(Data[i].X)\n",
    "    if options['shuffle']:\n",
    "        trainset, adaptset = random_split(fullset, [int(2/3*l), l-int(2/3*l)])\n",
    "    else:\n",
    "        trainset = mlmodel.MyDataset(Data[i].X[:int(2/3*l)], Data[i].Y[:int(2/3*l)], Data[i].C) \n",
    "        adaptset = mlmodel.MyDataset(Data[i].X[int(2/3*l):], Data[i].Y[int(2/3*l):], Data[i].C)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=options['phi_shot'], shuffle=options['shuffle'], num_workers=NUM_WORKERS)\n",
    "    adaptloader = torch.utils.data.DataLoader(adaptset, batch_size=options['K_shot'], shuffle=options['shuffle'], num_workers=NUM_WORKERS)\n",
    "   \n",
    "    # Trainset.append(trainset)\n",
    "    # Adaptset.append(adaptset)\n",
    "    Trainloader.append(trainloader) # for training phi\n",
    "    Adaptloader.append(adaptloader) # for LS on a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Adversarially Invariant Meta Learning\n",
    "\n",
    "Assume the state $x\\in\\mathbb{R}^n$ and $c$ is hidden state used to represent changing environment. We are interested in learning some function $f(x(t),c(t))$. $f(x(t),c(t))$ can be separated into three terms: $$f(x(t),c(t))=\\phi(x(t))a(c(t))+d(t),$$\n",
    "where $\\phi(x(t))$ captures the $c$-variant part and $a(c(t))\\in\\mathbb{R}^m$ is implicitly a function of the hidden state $c(t)$. Finally, $d(t)$ is the residual noise term.\n",
    "\n",
    "We want to learn $\\phi(x)$ such that it doesn't include any information about $c$. To reach this goal, we introduce another neural network $h$ where $h(\\phi(x))$ tries to predict $c$.\n",
    "\n",
    "The loss function is given as\n",
    "$$\\max_h\\min_{\\phi, \\left\\{a_{c_j}\\right\\}_j}\\sum_{j}\\sum_{i}\\left\\|\\phi(x^{(i)}_{c_j})a_{c_j}-f(x^{(i)}_{c_j},c_j)\\right\\|^2-\\alpha\\cdot\\text{CrossEntropy}\\left(h(\\phi(x^{(i)}_{c_j})),j\\right)$$\n",
    "Note that the $\\text{CrossEntropy-loss}$ will not require physical encoding of $c_j$ in training, only a label for $c$ that corresponds to the subdataset (that is, the label $c$ has no physical meaning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the model class definition in an external file so they can be referenced outside this script\n",
    "phi_net = mlmodel.Phi_Net(options)\n",
    "h_net = mlmodel.H_Net_CrossEntropy(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "criterion_h = nn.CrossEntropyLoss()\n",
    "optimizer_h = optim.Adam(h_net.parameters(), lr=options['learning_rate'])\n",
    "optimizer_phi = optim.Adam(phi_net.parameters(), lr=options['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-Training Algorithm\n",
    "\n",
    "**Step 0: sample $c$, and sample $B+K$ data points in correponding subdataset $\\{x_i,c,f(x_i,c)\\}_{i}$**\n",
    "\n",
    "**Step 1: estimate $a$ using least-square**\n",
    "\n",
    "$K$ data points (sampled from the same wind condition $c$) are used to compute $a$ using least-squares, i.e., adaptation:\n",
    "$$\n",
    "\\underbrace{\n",
    "    \\begin{bmatrix}\n",
    "        \\phi(x_1) \\\\ \\phi(x_2) \\\\ \\vdots \\\\ \\phi(x_K) \n",
    "    \\end{bmatrix}}\n",
    "    _{\\Phi\\in\\mathbb{R}^{K\\times \\dim(a)}}\n",
    "\\cdot\n",
    "\\underbrace{\n",
    "    \\begin{bmatrix}\n",
    "        a_1 & \\cdots & a_{\\dim(y)} \n",
    "    \\end{bmatrix}}\n",
    "    _{a\\in\\mathbb{R}^{\\dim(a)\\times \\dim(y)}}\n",
    "=\n",
    "\\underbrace{\n",
    "    \\begin{bmatrix}\n",
    "        f_1(x_1) & \\cdots & f_{\\dim(y)}(x_1) \\\\ f_1(x_2) & \\cdots & f_{\\dim(y)}(x_2) \\\\ \\vdots & \\vdots & \\vdots\\\\ f_1(x_K) & \\cdots & f_{\\dim(y)}(x_K) \n",
    "    \\end{bmatrix}}\n",
    "    _{Y\\in\\mathbb{R}^{K\\times \\dim(y)}}\n",
    "$$\n",
    "\n",
    "The least square solution is given by\n",
    "$$a=(\\Phi^\\top\\Phi)^{-1}\\Phi^\\top Y$$\n",
    "Normalization on $a$ is implemented to avoid ambiguity of $\\phi(x)a$ (since $\\phi(x)a=(0.1\\phi(x))\\cdot(10a)$):\n",
    "$$a\\leftarrow \\gamma\\cdot\\frac{a}{\\|a\\|_F},\\quad\\text{if}\\,\\,\\|a\\|_F>\\gamma$$\n",
    "Note that $a$ is an implicit function of $\\phi$.\n",
    "\n",
    "**Step 2: fix $h$ and train $\\phi$**\n",
    "\n",
    "With this $a$, another $B$ data points (with same $c$) are used for gradient descent with loss\n",
    "$$\\mathcal{L}(\\phi)=\\|f(x)-\\phi(x)a\\|_2^2-\\alpha\\cdot\\|h(\\phi(x))-c\\|_2^2$$\n",
    "\n",
    "**Step 3: fix $\\phi$ and train discriminator $h$**\n",
    "\n",
    "Finally, these $B$ data points are used again for gradient descent on $h$ with loss\n",
    "$$\\mathcal{L}(h)=\\|h(\\phi(x))-c\\|_2^2$$\n",
    "We may run this step less frequently than step 2, to improve stability in training (a trick from GAN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_save_freq = 50 # How often to save the model\n",
    "\n",
    "# Create some arrays to save training statistics\n",
    "Loss_f = [] # combined force prediction loss\n",
    "Loss_c = [] # combined adversarial loss\n",
    "\n",
    "# Loss for each subdataset \n",
    "Loss_test_nominal = [] # loss without any learning\n",
    "Loss_test_mean = [] # loss with mean predictor\n",
    "Loss_test_phi = [] # loss with NN\n",
    "for i in range(len(TestData)):\n",
    "    Loss_test_nominal.append([])\n",
    "    Loss_test_mean.append([])\n",
    "    Loss_test_phi.append([])\n",
    "\n",
    "# Training!\n",
    "for epoch in range(options['num_epochs']):\n",
    "    # Randomize the order in which we train over the subdatasets\n",
    "    arr = np.arange(options['num_c'])\n",
    "    np.random.shuffle(arr)\n",
    "\n",
    "    # Running loss over all subdatasets\n",
    "    running_loss_f = 0.0\n",
    "    running_loss_c = 0.0\n",
    "\n",
    "    for i in arr:\n",
    "        with torch.no_grad():\n",
    "            adaptloader = Adaptloader[i]\n",
    "            kshot_data = next(iter(adaptloader))\n",
    "            trainloader = Trainloader[i]\n",
    "            data = next(iter(trainloader))\n",
    "        \n",
    "        optimizer_phi.zero_grad()\n",
    "        \n",
    "        '''\n",
    "        Least-square to get $a$ from K-shot data\n",
    "        '''\n",
    "        X = kshot_data['input'] # K x dim_x\n",
    "        Y = kshot_data['output'] # K x dim_y\n",
    "        Phi = phi_net(X) # K x dim_a\n",
    "        Phi_T = Phi.transpose(0, 1) # dim_a x K\n",
    "        A = torch.inverse(torch.mm(Phi_T, Phi)) # dim_a x dim_a\n",
    "        a = torch.mm(torch.mm(A, Phi_T), Y) # dim_a x dim_y\n",
    "        if torch.norm(a, 'fro') > options['gamma']:\n",
    "            a = a / torch.norm(a, 'fro') * options['gamma']\n",
    "            \n",
    "        '''\n",
    "        Batch training \\phi_net\n",
    "        '''\n",
    "        inputs = data['input'] # B x dim_x\n",
    "        labels = data['output'] # B x dim_y\n",
    "        \n",
    "        c_labels = data['c'].type(torch.long)\n",
    "            \n",
    "        # forward + backward + optimize\n",
    "        outputs = torch.mm(phi_net(inputs), a)\n",
    "        loss_f = criterion(outputs, labels)\n",
    "        temp = phi_net(inputs)\n",
    "        \n",
    "        loss_c = criterion_h(h_net(temp), c_labels)\n",
    "            \n",
    "        loss_phi = loss_f - options['alpha'] * loss_c\n",
    "        loss_phi.backward()\n",
    "        optimizer_phi.step()\n",
    "        \n",
    "        '''\n",
    "        Discriminator training\n",
    "        '''\n",
    "        if np.random.rand() <= 1.0 / options['frequency_h']:\n",
    "            optimizer_h.zero_grad()\n",
    "            temp = phi_net(inputs)\n",
    "            \n",
    "            loss_c = criterion_h(h_net(temp), c_labels)\n",
    "            \n",
    "            loss_h = loss_c\n",
    "            loss_h.backward()\n",
    "            optimizer_h.step()\n",
    "        \n",
    "        '''\n",
    "        Spectral normalization\n",
    "        '''\n",
    "        if options['SN'] > 0:\n",
    "            for param in phi_net.parameters():\n",
    "                M = param.detach().numpy()\n",
    "                if M.ndim > 1:\n",
    "                    s = np.linalg.norm(M, 2)\n",
    "                    if s > options['SN']:\n",
    "                        param.data = param / s * options['SN']\n",
    "         \n",
    "        running_loss_f += loss_f.item()\n",
    "        running_loss_c += loss_c.item()\n",
    "    \n",
    "    # Save statistics\n",
    "    Loss_f.append(running_loss_f / options['num_c'])\n",
    "    Loss_c.append(running_loss_c / options['num_c'])\n",
    "    if epoch % 10 == 0:\n",
    "        print('[%d] loss_f: %.2f loss_c: %.2f' % (epoch + 1, running_loss_f / options['num_c'], running_loss_c / options['num_c']))\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for j in range(len(TestData)):\n",
    "            loss_nominal, loss_mean, loss_phi = mlmodel.error_statistics(TestData[j].X, TestData[j].Y, phi_net, h_net, options=options)\n",
    "            Loss_test_nominal[j].append(loss_nominal)\n",
    "            Loss_test_mean[j].append(loss_mean)\n",
    "            Loss_test_phi[j].append(loss_phi)\n",
    "\n",
    "    if epoch % model_save_freq == 0:\n",
    "        mlmodel.save_model(phi_net=phi_net, h_net=h_net, modelname=modelname + '-epoch-' + str(epoch), options=options)\n",
    "\n",
    "mlmodel.save_model(phi_net=phi_net, h_net=h_net, modelname=modelname + '-epoch-' + str(options[\"num_epochs\"]), options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(Loss_f)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('f-loss [N]')\n",
    "plt.title('training f loss')\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(Loss_c)\n",
    "plt.title('training c loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('c-loss')\n",
    "plt.tight_layout()\n",
    "\n",
    "for j in range(len(TestData)):\n",
    "    plt.figure()\n",
    "    # plt.plot(Loss_test_nominal[j], label='nominal')\n",
    "    plt.plot(Loss_test_mean[j], label='mean')\n",
    "    plt.plot(np.array(Loss_test_phi[j]), label='phi*a')\n",
    "    # plt.plot(np.array(Loss_test_exp_forgetting[j]), label='exp forgetting')\n",
    "    plt.legend()\n",
    "    plt.title(f'Test data set {j} - {TestData[j].meta[\"condition\"]}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose final model\n",
    "# stopping_epoch = 200\n",
    "# options['num_epochs'] = stopping_epoch\n",
    "\n",
    "model_path = os.path.join(options[\"output_path\"], \"models\", (modelname + '-epoch-' + str(options[\"num_epochs\"])))\n",
    "# model_path = os.path.join(\"/home/hcr/Research/DARoSLab/rina/training_results/04_02_202415_31_57/models\", (modelname + '-epoch-' + str(2000)))\n",
    "final_model = mlmodel.load_model(modelname = model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_net = final_model.phi\n",
    "h_net = final_model.h\n",
    "\n",
    "eval_adapt_start = 0\n",
    "eval_adapt_end = 2500\n",
    "eval_val_start = 2500\n",
    "eval_val_end = 5000\n",
    "\n",
    "vis_output_path_prefix_training_data = os.path.join(options[\"output_path\"], \"eval_iamges\", \"training\")\n",
    "vis_output_path_prefix_testing_data = os.path.join(options[\"output_path\"], \"eval_iamges\", \"testing\")\n",
    "\n",
    "# vis_output_path_prefix_training_data = os.path.join(\"/home/hcr/Research/DARoSLab/rina/training_results/04_02_202415_31_57/eval_images/training\")\n",
    "# vis_output_path_prefix_testing_data = os.path.join(\"/home/hcr/Research/DARoSLab/rina/training_results/04_02_202415_31_57/eval_images/testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the measured aerodynamic force, labeled ground truth (gt), along with the region used for adapation (adapt), and the predicted region (val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(Data):\n",
    "    print('------------------------------')\n",
    "    print(data.meta['condition'] + ':')\n",
    "    file_name = \"{:s}.png\".format(data.meta['condition'])\n",
    "    mlmodel.vis_validation(t=data.meta['steps'], x=data.X, y=data.Y, phi_net=phi_net, \n",
    "                           h_net=h_net, idx_adapt_start=eval_adapt_start, idx_adapt_end=eval_adapt_end, \n",
    "                           idx_val_start=eval_val_start, idx_val_end=eval_val_end, c=Data[i].C, options=options, \n",
    "                           output_path_prefix=vis_output_path_prefix_training_data, output_name=file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in Data:\n",
    "    image_name = \"{:s}_errors_hist.png\".format(data.meta['condition'])\n",
    "    error_1, error_2, error_3 = mlmodel.error_statistics_hist(data.X, data.Y, phi_net, h_net, options, vis_output_path_prefix_training_data, image_name)\n",
    "    print('**** c =', str(data.C), ':', data.meta['condition'], '****')\n",
    "    print(f'Before learning: MSE is {error_1: .2f}')\n",
    "    print(f'Mean predictor: MSE is {error_2: .2f}')\n",
    "    print(f'After learning phi(x): MSE is {error_3: .2f}')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(TestData):\n",
    "    print('------------------------------')\n",
    "    print(data.meta['condition'] + ':')\n",
    "    print(len(data.X))\n",
    "    file_name = \"{:s}.png\".format(data.meta['condition'])\n",
    "    mlmodel.vis_validation(t=data.meta['steps'], x=data.X, y=data.Y, phi_net=phi_net, h_net=h_net, \n",
    "                           idx_adapt_start=eval_adapt_start, idx_adapt_end=eval_adapt_end, \n",
    "                           idx_val_start=eval_val_start, idx_val_end=eval_val_end, c=TestData[i].C, options=options,\n",
    "                           output_path_prefix=vis_output_path_prefix_testing_data, output_name=file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in TestData:\n",
    "    image_name = \"{:s}_errors_hist.png\".format(data.meta['condition'])\n",
    "    error_1, error_2, error_3 = mlmodel.error_statistics_hist(data.X, data.Y, phi_net, h_net, options, vis_output_path_prefix_testing_data, image_name)\n",
    "    print('**** :', data.meta['condition'], '****')\n",
    "    print(f'Before learning: MSE is {error_1: .2f}')\n",
    "    print(f'Mean predictor: MSE is {error_2: .2f}')\n",
    "    print(f'After learning phi(x): MSE is {error_3: .2f}')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5932d0aacfea12ae558253bdaac47ff94f362ee7b8ad1d98be4f4f6b94d2042e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "metadata": {
   "interpreter": {
    "hash": "efbe968048790da5e26fe224257c59db0c3c3cb10bbad9ca12250f2d56e94a61"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
