/home/oyoungquist/.conda/envs/rina/lib/python3.11/site-packages/torch/__init__.py:747: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608853085/work/torch/csrc/tensor/python_tensor.cpp:431.)
  _C._set_default_tensor_type(t)
{'train_path': '/home/oyoungquist/Research/RINA/rina/data/lcm_converted_log/06_24_2024_formal/training_data_corrected/', 'test_path': '/home/oyoungquist/Research/RINA/rina/data/lcm_converted_log/06_24_2024_formal/eval_data_corrected/', 'num_epochs': 10000, 'learning_rate': 0.0009, 'model_save_freq': 100, 'SN': 6.0, 'gamma': 10, 'alpha': 0.048, 'frequency_h': 2.0, 'phi_first_out': 128, 'phi_second_out': 128, 'discrim_hidden': 64, 'K_shot': 50, 'phi_shot': 2048, 'device': 'cuda:0', 'shuffle': True, 'save_data_plots': True, 'display_progress': True, 'features': ['q', 'q_dot', 'tau_cmd'], 'label': 'tau_residual_cmd_centered', 'dim_a': 16, 'output_prefix': 'cmd_residual_centered_c'}



{'dim_a': 16, 'features': ['q', 'q_dot', 'tau_cmd'], 'label': 'tau_residual_cmd', 'labels': ['FR_hip', 'FR_knee', 'FR_foot', 'FL_hip', 'FL_knee', 'FL_foot', 'RR_hip', 'RR_knee', 'RR_foot', 'RL_hip', 'RL_knee', 'RL_foot'], 'dataset': 'rina', 'learning_rate': 0.0005, 'model_save_freq': 100, 'num_epochs': 500, 'gamma': 10, 'alpha': 0.01, 'frequency_h': 2.0, 'SN': 4, 'train_path': '/home/oyoungquist/Research/RINA/rina/data/lcm_converted_log/05_17_2024_formal/training_data_ex/', 'test_path': '/home/oyoungquist/Research/RINA/rina/data/lcm_converted_log/05_17_2024_formal/eval_data_ex/', 'shuffle': True, 'body_offset': 0, 'K_shot': 32, 'phi_shot': 256, 'loss_type': 'crossentropy-loss', 'phi_first_out': 128, 'phi_second_out': 128, 'discrim_hidden': 20, 'display_progress': False}
{'dim_a': 16, 'features': ['q', 'q_dot', 'tau_cmd'], 'label': 'tau_residual_cmd_centered', 'labels': ['FR_hip', 'FR_knee', 'FR_foot', 'FL_hip', 'FL_knee', 'FL_foot', 'RR_hip', 'RR_knee', 'RR_foot', 'RL_hip', 'RL_knee', 'RL_foot'], 'dataset': 'rina', 'learning_rate': 0.0009, 'model_save_freq': 100, 'num_epochs': 10000, 'gamma': 10, 'alpha': 0.048, 'frequency_h': 2.0, 'SN': 6.0, 'train_path': '/home/oyoungquist/Research/RINA/rina/data/lcm_converted_log/06_24_2024_formal/training_data_corrected/', 'test_path': '/home/oyoungquist/Research/RINA/rina/data/lcm_converted_log/06_24_2024_formal/eval_data_corrected/', 'shuffle': True, 'body_offset': 0, 'K_shot': 50, 'phi_shot': 2048, 'loss_type': 'crossentropy-loss', 'phi_first_out': 128, 'phi_second_out': 128, 'discrim_hidden': 64, 'display_progress': True, 'device': 'cuda:0', 'save_data_plots': True, 'output_prefix': 'cmd_residual_centered_c'}
running on linux, setting 2 workers

***********Model output path:  /home/oyoungquist/Research/RINA/rina/training_results/cmd_residual_centered_c/06_30_202421_17_43_cmd_res_cc_128_128_a16_h64_e10000
[1] loss_f: 7.06 loss_c: 1.11
[11] loss_f: 6.38 loss_c: 1.11
[21] loss_f: 6.02 loss_c: 1.11
[31] loss_f: 5.12 loss_c: 1.11
[41] loss_f: 4.12 loss_c: 1.10
[51] loss_f: 4.82 loss_c: 1.09
[61] loss_f: 4.72 loss_c: 1.09
[71] loss_f: 4.21 loss_c: 1.09
[81] loss_f: 4.27 loss_c: 1.09
[91] loss_f: 4.52 loss_c: 1.10
[101] loss_f: 4.15 loss_c: 1.09
[111] loss_f: 3.95 loss_c: 1.09
[121] loss_f: 3.97 loss_c: 1.10
[131] loss_f: 3.96 loss_c: 1.09
[141] loss_f: 3.38 loss_c: 1.10
[151] loss_f: 4.01 loss_c: 1.09
[161] loss_f: 3.79 loss_c: 1.09
[171] loss_f: 3.45 loss_c: 1.07
[181] loss_f: 3.39 loss_c: 1.09
[191] loss_f: 3.27 loss_c: 1.08
[201] loss_f: 4.20 loss_c: 1.09
[211] loss_f: 3.38 loss_c: 1.07
[221] loss_f: 3.50 loss_c: 1.07
[231] loss_f: 3.12 loss_c: 1.07
[241] loss_f: 4.11 loss_c: 1.07
[251] loss_f: 3.21 loss_c: 1.08
Traceback (most recent call last):
  File "/home/oyoungquist/.conda/envs/rina/lib/python3.11/multiprocessing/util.py", line 303, in _run_finalizers
    finalizer()
  File "/home/oyoungquist/.conda/envs/rina/lib/python3.11/multiprocessing/util.py", line 227, in __call__
    res = self._callback(*self._args, **self._kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/oyoungquist/.conda/envs/rina/lib/python3.11/multiprocessing/util.py", line 136, in _remove_temp_dir
    rmtree(tempdir, onerror=onerror)
  File "/home/oyoungquist/.conda/envs/rina/lib/python3.11/shutil.py", line 763, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/home/oyoungquist/.conda/envs/rina/lib/python3.11/shutil.py", line 761, in rmtree
    os.rmdir(path, dir_fd=dir_fd)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-l4oh3pg6'
[261] loss_f: 3.16 loss_c: 1.08
[271] loss_f: 2.84 loss_c: 1.08
[281] loss_f: 2.99 loss_c: 1.07
[291] loss_f: 3.06 loss_c: 1.08
[301] loss_f: 3.22 loss_c: 1.09
[311] loss_f: 3.90 loss_c: 1.09
[321] loss_f: 4.15 loss_c: 1.08
[331] loss_f: 3.55 loss_c: 1.08
[341] loss_f: 2.92 loss_c: 1.08
[351] loss_f: 3.11 loss_c: 1.07
[361] loss_f: 2.76 loss_c: 1.08
[371] loss_f: 2.93 loss_c: 1.09
[381] loss_f: 3.56 loss_c: 1.10
[391] loss_f: 3.04 loss_c: 1.09
[401] loss_f: 3.15 loss_c: 1.07
[411] loss_f: 3.04 loss_c: 1.08
[421] loss_f: 2.77 loss_c: 1.07
[431] loss_f: 2.93 loss_c: 1.08
[441] loss_f: 3.02 loss_c: 1.09
[451] loss_f: 3.00 loss_c: 1.09
[461] loss_f: 4.72 loss_c: 1.09
[471] loss_f: 2.58 loss_c: 1.09
[481] loss_f: 3.08 loss_c: 1.07
[491] loss_f: 2.67 loss_c: 1.08
[501] loss_f: 2.94 loss_c: 1.08
[511] loss_f: 2.49 loss_c: 1.08
[521] loss_f: 3.04 loss_c: 1.07
[531] loss_f: 3.08 loss_c: 1.08
[541] loss_f: 3.16 loss_c: 1.08
[551] loss_f: 2.85 loss_c: 1.07
[561] loss_f: 3.18 loss_c: 1.08
[571] loss_f: 3.10 loss_c: 1.08
[581] loss_f: 2.51 loss_c: 1.07
[591] loss_f: 2.66 loss_c: 1.07
[601] loss_f: 2.57 loss_c: 1.07
[611] loss_f: 3.19 loss_c: 1.07
[621] loss_f: 2.66 loss_c: 1.07
[631] loss_f: 2.62 loss_c: 1.07
[641] loss_f: 2.52 loss_c: 1.06
[651] loss_f: 3.39 loss_c: 1.07
[661] loss_f: 2.68 loss_c: 1.07
[671] loss_f: 2.76 loss_c: 1.06
[681] loss_f: 2.39 loss_c: 1.06
[691] loss_f: 2.79 loss_c: 1.07
[701] loss_f: 2.89 loss_c: 1.06
[711] loss_f: 2.23 loss_c: 1.07
[721] loss_f: 2.57 loss_c: 1.06
[731] loss_f: 2.32 loss_c: 1.07
[741] loss_f: 2.35 loss_c: 1.06
[751] loss_f: 2.54 loss_c: 1.06
[761] loss_f: 2.36 loss_c: 1.05
[771] loss_f: 2.60 loss_c: 1.06
[781] loss_f: 2.36 loss_c: 1.06
[791] loss_f: 2.55 loss_c: 1.07
[801] loss_f: 2.65 loss_c: 1.08
[811] loss_f: 3.05 loss_c: 1.07
[821] loss_f: 2.61 loss_c: 1.06
[831] loss_f: 2.33 loss_c: 1.06
[841] loss_f: 2.58 loss_c: 1.05
[851] loss_f: 2.46 loss_c: 1.04
[861] loss_f: 2.70 loss_c: 1.06
[871] loss_f: 3.04 loss_c: 1.05
[881] loss_f: 2.74 loss_c: 1.05
[891] loss_f: 2.39 loss_c: 1.06
[901] loss_f: 2.46 loss_c: 1.05
[911] loss_f: 2.67 loss_c: 1.06
[921] loss_f: 3.01 loss_c: 1.06
[931] loss_f: 2.74 loss_c: 1.06
[941] loss_f: 2.48 loss_c: 1.07
[951] loss_f: 2.44 loss_c: 1.07
[961] loss_f: 2.39 loss_c: 1.05
[971] loss_f: 2.29 loss_c: 1.05
[981] loss_f: 3.26 loss_c: 1.05
[991] loss_f: 3.03 loss_c: 1.05
[1001] loss_f: 2.40 loss_c: 1.05
[1011] loss_f: 3.13 loss_c: 1.05
[1021] loss_f: 2.28 loss_c: 1.06
[1031] loss_f: 2.39 loss_c: 1.05
[1041] loss_f: 2.54 loss_c: 1.05
[1051] loss_f: 2.51 loss_c: 1.04
[1061] loss_f: 2.50 loss_c: 1.04
[1071] loss_f: 3.18 loss_c: 1.04
[1081] loss_f: 2.48 loss_c: 1.04
[1091] loss_f: 2.40 loss_c: 1.04
[1101] loss_f: 2.13 loss_c: 1.04
[1111] loss_f: 2.42 loss_c: 1.04
[1121] loss_f: 2.42 loss_c: 1.03
[1131] loss_f: 2.44 loss_c: 1.03
[1141] loss_f: 2.51 loss_c: 1.03
[1151] loss_f: 2.56 loss_c: 1.04
[1161] loss_f: 2.51 loss_c: 1.04
[1171] loss_f: 2.40 loss_c: 1.03
[1181] loss_f: 2.25 loss_c: 1.04
[1191] loss_f: 2.09 loss_c: 1.04
[1201] loss_f: 2.75 loss_c: 1.05
[1211] loss_f: 2.51 loss_c: 1.04
[1221] loss_f: 2.38 loss_c: 1.03
[1231] loss_f: 2.28 loss_c: 1.04
[1241] loss_f: 2.62 loss_c: 1.04
[1251] loss_f: 2.21 loss_c: 1.04
[1261] loss_f: 2.35 loss_c: 1.04
[1271] loss_f: 2.21 loss_c: 1.03
[1281] loss_f: 2.50 loss_c: 1.03
[1291] loss_f: 2.12 loss_c: 1.03
[1301] loss_f: 2.17 loss_c: 1.03
[1311] loss_f: 2.18 loss_c: 1.05
[1321] loss_f: 2.15 loss_c: 1.04
[1331] loss_f: 2.11 loss_c: 1.05
[1341] loss_f: 2.32 loss_c: 1.06
[1351] loss_f: 2.22 loss_c: 1.05
[1361] loss_f: 2.09 loss_c: 1.05
[1371] loss_f: 2.70 loss_c: 1.06
[1381] loss_f: 2.22 loss_c: 1.05
[1391] loss_f: 2.21 loss_c: 1.04
[1401] loss_f: 2.43 loss_c: 1.06
[1411] loss_f: 2.53 loss_c: 1.16
[1421] loss_f: 3.09 loss_c: 1.10
[1431] loss_f: 2.43 loss_c: 1.16
[1441] loss_f: 2.62 loss_c: 1.07
[1451] loss_f: 2.25 loss_c: 1.07
[1461] loss_f: 2.28 loss_c: 1.07
[1471] loss_f: 2.14 loss_c: 1.04
[1481] loss_f: 2.10 loss_c: 1.03
[1491] loss_f: 2.52 loss_c: 1.03
[1501] loss_f: 2.12 loss_c: 1.04
[1511] loss_f: 2.14 loss_c: 1.03
[1521] loss_f: 2.11 loss_c: 1.05
[1531] loss_f: 2.08 loss_c: 1.05
[1541] loss_f: 2.58 loss_c: 1.03
[1551] loss_f: 2.78 loss_c: 1.03
[1561] loss_f: 2.07 loss_c: 1.05
[1571] loss_f: 2.44 loss_c: 1.04
[1581] loss_f: 2.21 loss_c: 1.04
[1591] loss_f: 2.62 loss_c: 1.05
[1601] loss_f: 2.08 loss_c: 1.04
[1611] loss_f: 2.37 loss_c: 1.03
[1621] loss_f: 2.09 loss_c: 1.03
[1631] loss_f: 2.25 loss_c: 1.03
[1641] loss_f: 2.07 loss_c: 1.03
[1651] loss_f: 2.04 loss_c: 1.03
[1661] loss_f: 2.16 loss_c: 1.04
[1671] loss_f: 2.28 loss_c: 1.03
[1681] loss_f: 2.11 loss_c: 1.03
[1691] loss_f: 2.21 loss_c: 1.03
[1701] loss_f: 2.14 loss_c: 1.05
[1711] loss_f: 2.49 loss_c: 1.05
[1721] loss_f: 2.54 loss_c: 1.04
[1731] loss_f: 2.29 loss_c: 1.02
[1741] loss_f: 2.31 loss_c: 1.03
[1751] loss_f: 2.02 loss_c: 1.03
[1761] loss_f: 2.24 loss_c: 1.03
[1771] loss_f: 2.11 loss_c: 1.02
Traceback (most recent call last):
  File "/home/oyoungquist/.conda/envs/rina/lib/python3.11/multiprocessing/util.py", line 303, in _run_finalizers
    finalizer()
  File "/home/oyoungquist/.conda/envs/rina/lib/python3.11/multiprocessing/util.py", line 227, in __call__
    res = self._callback(*self._args, **self._kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/oyoungquist/.conda/envs/rina/lib/python3.11/multiprocessing/util.py", line 136, in _remove_temp_dir
    rmtree(tempdir, onerror=onerror)
  File "/home/oyoungquist/.conda/envs/rina/lib/python3.11/shutil.py", line 763, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/home/oyoungquist/.conda/envs/rina/lib/python3.11/shutil.py", line 761, in rmtree
    os.rmdir(path, dir_fd=dir_fd)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-9946rpu8'
[1781] loss_f: 2.08 loss_c: 1.02
[1791] loss_f: 1.92 loss_c: 1.02
[1801] loss_f: 2.11 loss_c: 1.02
[1811] loss_f: 2.35 loss_c: 1.04
[1821] loss_f: 2.15 loss_c: 1.04
[1831] loss_f: 2.31 loss_c: 1.02
[1841] loss_f: 2.26 loss_c: 1.03
[1851] loss_f: 2.27 loss_c: 1.02
[1861] loss_f: 2.48 loss_c: 1.01
[1871] loss_f: 2.17 loss_c: 1.05
[1881] loss_f: 1.97 loss_c: 1.06
[1891] loss_f: 2.34 loss_c: 1.03
[1901] loss_f: 2.24 loss_c: 1.06
[1911] loss_f: 2.32 loss_c: 1.06
[1921] loss_f: 2.21 loss_c: 1.06
[1931] loss_f: 2.13 loss_c: 1.07
[1941] loss_f: 2.29 loss_c: 1.03
[1951] loss_f: 2.16 loss_c: 1.04
[1961] loss_f: 1.99 loss_c: 1.02
[1971] loss_f: 2.11 loss_c: 1.03
[1981] loss_f: 2.03 loss_c: 1.03
[1991] loss_f: 2.11 loss_c: 1.03
[2001] loss_f: 1.83 loss_c: 1.02
[2011] loss_f: 2.22 loss_c: 1.03
[2021] loss_f: 2.05 loss_c: 1.02
[2031] loss_f: 2.16 loss_c: 1.04
[2041] loss_f: 2.45 loss_c: 1.04
[2051] loss_f: 2.22 loss_c: 1.04
[2061] loss_f: 2.18 loss_c: 1.03
[2071] loss_f: 1.96 loss_c: 1.03
[2081] loss_f: 2.00 loss_c: 1.01
[2091] loss_f: 2.61 loss_c: 1.01
[2101] loss_f: 1.94 loss_c: 1.01
[2111] loss_f: 1.95 loss_c: 1.02
[2121] loss_f: 2.21 loss_c: 1.01
[2131] loss_f: 1.99 loss_c: 1.00
[2141] loss_f: 2.00 loss_c: 1.00
[2151] loss_f: 1.97 loss_c: 1.01
[2161] loss_f: 2.13 loss_c: 1.00
[2171] loss_f: 2.25 loss_c: 1.00
[2181] loss_f: 2.28 loss_c: 1.01
[2191] loss_f: 2.86 loss_c: 1.01
[2201] loss_f: 2.17 loss_c: 1.02
[2211] loss_f: 2.46 loss_c: 1.01
[2221] loss_f: 2.01 loss_c: 1.01
[2231] loss_f: 1.90 loss_c: 1.03
[2241] loss_f: 1.84 loss_c: 1.04
[2251] loss_f: 2.18 loss_c: 1.04
[2261] loss_f: 1.99 loss_c: 1.05
[2271] loss_f: 2.09 loss_c: 1.02
[2281] loss_f: 2.23 loss_c: 1.04
[2291] loss_f: 2.24 loss_c: 1.02
[2301] loss_f: 1.84 loss_c: 1.00
[2311] loss_f: 1.97 loss_c: 1.00
[2321] loss_f: 2.23 loss_c: 1.01
[2331] loss_f: 2.06 loss_c: 1.01
[2341] loss_f: 2.16 loss_c: 1.02
[2351] loss_f: 2.14 loss_c: 1.03
[2361] loss_f: 2.15 loss_c: 1.03
[2371] loss_f: 2.38 loss_c: 1.00
[2381] loss_f: 2.12 loss_c: 1.00
[2391] loss_f: 2.22 loss_c: 1.02
[2401] loss_f: 2.14 loss_c: 1.00
[2411] loss_f: 2.22 loss_c: 1.01
[2421] loss_f: 2.08 loss_c: 1.01
[2431] loss_f: 1.90 loss_c: 1.01
[2441] loss_f: 2.27 loss_c: 1.01
[2451] loss_f: 2.11 loss_c: 1.02
[2461] loss_f: 2.14 loss_c: 1.02
[2471] loss_f: 2.19 loss_c: 1.00
[2481] loss_f: 2.18 loss_c: 1.00
[2491] loss_f: 2.28 loss_c: 1.00
[2501] loss_f: 1.95 loss_c: 1.01
[2511] loss_f: 2.22 loss_c: 1.02
[2521] loss_f: 2.05 loss_c: 1.02
[2531] loss_f: 1.93 loss_c: 1.03
[2541] loss_f: 1.98 loss_c: 1.02
[2551] loss_f: 1.93 loss_c: 1.01
[2561] loss_f: 1.96 loss_c: 1.00
[2571] loss_f: 2.02 loss_c: 1.00
[2581] loss_f: 1.98 loss_c: 1.02
[2591] loss_f: 1.84 loss_c: 1.01
[2601] loss_f: 1.94 loss_c: 1.03
[2611] loss_f: 1.96 loss_c: 1.06
[2621] loss_f: 2.06 loss_c: 1.01
[2631] loss_f: 1.88 loss_c: 1.02
[2641] loss_f: 2.08 loss_c: 1.01
Traceback (most recent call last):
  File "/home/oyoungquist/.conda/envs/rina/lib/python3.11/multiprocessing/util.py", line 303, in _run_finalizers
    finalizer()
  File "/home/oyoungquist/.conda/envs/rina/lib/python3.11/multiprocessing/util.py", line 227, in __call__
    res = self._callback(*self._args, **self._kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/oyoungquist/.conda/envs/rina/lib/python3.11/multiprocessing/util.py", line 136, in _remove_temp_dir
    rmtree(tempdir, onerror=onerror)
  File "/home/oyoungquist/.conda/envs/rina/lib/python3.11/shutil.py", line 763, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/home/oyoungquist/.conda/envs/rina/lib/python3.11/shutil.py", line 761, in rmtree
    os.rmdir(path, dir_fd=dir_fd)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-krc9yhw9'
[2651] loss_f: 2.01 loss_c: 1.03
[2661] loss_f: 2.23 loss_c: 1.02
[2671] loss_f: 2.28 loss_c: 1.00
[2681] loss_f: 1.83 loss_c: 1.02
[2691] loss_f: 1.96 loss_c: 1.01
[2701] loss_f: 2.19 loss_c: 0.99
[2711] loss_f: 1.94 loss_c: 0.99
[2721] loss_f: 2.07 loss_c: 0.99
[2731] loss_f: 1.87 loss_c: 1.02
[2741] loss_f: 2.07 loss_c: 1.01
[2751] loss_f: 2.13 loss_c: 1.00
[2761] loss_f: 2.13 loss_c: 1.00
[2771] loss_f: 2.01 loss_c: 1.01
[2781] loss_f: 1.98 loss_c: 1.00
[2791] loss_f: 1.81 loss_c: 0.99
[2801] loss_f: 1.98 loss_c: 1.01
[2811] loss_f: 1.91 loss_c: 1.00
[2821] loss_f: 2.02 loss_c: 1.01
[2831] loss_f: 1.88 loss_c: 1.00
[2841] loss_f: 2.11 loss_c: 1.01
[2851] loss_f: 1.75 loss_c: 0.99
[2861] loss_f: 1.78 loss_c: 1.00
[2871] loss_f: 1.83 loss_c: 1.01
[2881] loss_f: 2.01 loss_c: 0.99
[2891] loss_f: 1.90 loss_c: 1.00
[2901] loss_f: 1.94 loss_c: 0.99
[2911] loss_f: 1.80 loss_c: 1.01
[2921] loss_f: 2.12 loss_c: 1.00
[2931] loss_f: 1.84 loss_c: 1.00
[2941] loss_f: 2.06 loss_c: 1.01
[2951] loss_f: 1.73 loss_c: 1.03
[2961] loss_f: 2.50 loss_c: 1.03
[2971] loss_f: 1.91 loss_c: 1.00
[2981] loss_f: 2.10 loss_c: 1.01
[2991] loss_f: 1.88 loss_c: 1.01
[3001] loss_f: 1.72 loss_c: 1.02
[3011] loss_f: 1.68 loss_c: 1.00
[3021] loss_f: 1.76 loss_c: 0.99
[3031] loss_f: 1.79 loss_c: 1.00
[3041] loss_f: 1.82 loss_c: 1.02
[3051] loss_f: 1.71 loss_c: 1.00
[3061] loss_f: 1.89 loss_c: 1.01
[3071] loss_f: 1.95 loss_c: 1.00
[3081] loss_f: 1.98 loss_c: 0.99
[3091] loss_f: 2.10 loss_c: 0.99
[3101] loss_f: 1.73 loss_c: 0.99
[3111] loss_f: 2.03 loss_c: 1.00
[3121] loss_f: 1.64 loss_c: 1.01
[3131] loss_f: 1.89 loss_c: 1.00
[3141] loss_f: 2.41 loss_c: 1.04
Traceback (most recent call last):
  File "/home/oyoungquist/.conda/envs/rina/lib/python3.11/multiprocessing/util.py", line 303, in _run_finalizers
    finalizer()
  File "/home/oyoungquist/.conda/envs/rina/lib/python3.11/multiprocessing/util.py", line 227, in __call__
    res = self._callback(*self._args, **self._kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/oyoungquist/.conda/envs/rina/lib/python3.11/multiprocessing/util.py", line 136, in _remove_temp_dir
    rmtree(tempdir, onerror=onerror)
  File "/home/oyoungquist/.conda/envs/rina/lib/python3.11/shutil.py", line 763, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/home/oyoungquist/.conda/envs/rina/lib/python3.11/shutil.py", line 761, in rmtree
    os.rmdir(path, dir_fd=dir_fd)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-qexysk3p'
[3151] loss_f: 1.81 loss_c: 1.03
[3161] loss_f: 2.16 loss_c: 1.01
[3171] loss_f: 1.65 loss_c: 1.01
[3181] loss_f: 1.96 loss_c: 1.02
[3191] loss_f: 2.15 loss_c: 0.99
[3201] loss_f: 1.93 loss_c: 1.09
[3211] loss_f: 1.71 loss_c: 1.02
[3221] loss_f: 1.79 loss_c: 1.02
[3231] loss_f: 1.87 loss_c: 1.02
[3241] loss_f: 2.13 loss_c: 1.01
[3251] loss_f: 1.98 loss_c: 1.03
[3261] loss_f: 1.75 loss_c: 1.02
[3271] loss_f: 2.45 loss_c: 1.01
[3281] loss_f: 1.93 loss_c: 1.02
[3291] loss_f: 1.90 loss_c: 1.00
[3301] loss_f: 1.85 loss_c: 1.00
[3311] loss_f: 2.18 loss_c: 1.02
[3321] loss_f: 2.00 loss_c: 1.01
[3331] loss_f: 1.79 loss_c: 1.02
[3341] loss_f: 1.84 loss_c: 1.01
[3351] loss_f: 2.03 loss_c: 1.01
[3361] loss_f: 2.27 loss_c: 1.01
[3371] loss_f: 2.31 loss_c: 1.16
[3381] loss_f: 1.92 loss_c: 1.03
[3391] loss_f: 2.03 loss_c: 1.01
[3401] loss_f: 2.66 loss_c: 1.00
[3411] loss_f: 1.63 loss_c: 1.00
[3421] loss_f: 2.41 loss_c: 0.99
[3431] loss_f: 1.79 loss_c: 1.01
[3441] loss_f: 1.94 loss_c: 1.03
[3451] loss_f: 1.99 loss_c: 1.00
[3461] loss_f: 1.86 loss_c: 0.99
[3471] loss_f: 1.86 loss_c: 1.00
[3481] loss_f: 2.01 loss_c: 1.00
[3491] loss_f: 2.00 loss_c: 1.00
[3501] loss_f: 2.25 loss_c: 1.00
[3511] loss_f: 2.67 loss_c: 1.00
[3521] loss_f: 1.89 loss_c: 1.00
[3531] loss_f: 1.87 loss_c: 0.99
[3541] loss_f: 1.50 loss_c: 0.98
[3551] loss_f: 2.00 loss_c: 0.99
[3561] loss_f: 1.82 loss_c: 0.98
[3571] loss_f: 1.82 loss_c: 1.01
[3581] loss_f: 1.92 loss_c: 1.00
[3591] loss_f: 1.80 loss_c: 1.00
[3601] loss_f: 1.83 loss_c: 1.00
[3611] loss_f: 1.74 loss_c: 1.00
[3621] loss_f: 1.85 loss_c: 0.99
[3631] loss_f: 1.83 loss_c: 1.02
[3641] loss_f: 1.92 loss_c: 0.99
[3651] loss_f: 1.69 loss_c: 0.99
[3661] loss_f: 1.67 loss_c: 1.00
[3671] loss_f: 1.71 loss_c: 0.99
[3681] loss_f: 1.66 loss_c: 1.02
[3691] loss_f: 1.99 loss_c: 1.01
[3701] loss_f: 2.52 loss_c: 1.05
[3711] loss_f: 2.19 loss_c: 1.02
[3721] loss_f: 2.04 loss_c: 0.99
[3731] loss_f: 1.88 loss_c: 1.00
[3741] loss_f: 1.74 loss_c: 1.00
[3751] loss_f: 1.79 loss_c: 0.98
[3761] loss_f: 2.01 loss_c: 1.00
[3771] loss_f: 1.88 loss_c: 0.99
[3781] loss_f: 1.95 loss_c: 1.00
[3791] loss_f: 1.90 loss_c: 0.97
[3801] loss_f: 1.92 loss_c: 1.01
[3811] loss_f: 1.80 loss_c: 1.04
[3821] loss_f: 1.87 loss_c: 1.10
[3831] loss_f: 2.10 loss_c: 1.02
[3841] loss_f: 1.86 loss_c: 1.07
[3851] loss_f: 1.62 loss_c: 1.05
[3861] loss_f: 1.96 loss_c: 1.03
[3871] loss_f: 1.78 loss_c: 1.03
[3881] loss_f: 1.68 loss_c: 1.02
[3891] loss_f: 1.84 loss_c: 1.03
[3901] loss_f: 1.84 loss_c: 1.00
[3911] loss_f: 2.08 loss_c: 1.02
[3921] loss_f: 1.90 loss_c: 1.01
[3931] loss_f: 1.83 loss_c: 1.03
[3941] loss_f: 1.60 loss_c: 1.04
[3951] loss_f: 1.90 loss_c: 1.01
[3961] loss_f: 2.09 loss_c: 1.01
[3971] loss_f: 2.06 loss_c: 1.02
[3981] loss_f: 1.61 loss_c: 1.00
[3991] loss_f: 2.03 loss_c: 1.09
[4001] loss_f: 3.05 loss_c: 1.05
[4011] loss_f: 1.99 loss_c: 1.02
[4021] loss_f: 1.69 loss_c: 0.99
[4031] loss_f: 1.80 loss_c: 1.03
[4041] loss_f: 2.54 loss_c: 1.01
[4051] loss_f: 1.67 loss_c: 1.00
[4061] loss_f: 1.60 loss_c: 1.00
[4071] loss_f: 1.81 loss_c: 1.00
[4081] loss_f: 1.81 loss_c: 1.01
[4091] loss_f: 1.87 loss_c: 1.03
[4101] loss_f: 1.56 loss_c: 1.06
[4111] loss_f: 1.71 loss_c: 1.02
[4121] loss_f: 1.78 loss_c: 1.02
[4131] loss_f: 1.94 loss_c: 1.01
[4141] loss_f: 1.76 loss_c: 1.03
[4151] loss_f: 1.89 loss_c: 1.02
[4161] loss_f: 1.90 loss_c: 1.00
[4171] loss_f: 1.76 loss_c: 1.00
[4181] loss_f: 1.55 loss_c: 0.99
[4191] loss_f: 1.67 loss_c: 1.03
[4201] loss_f: 1.63 loss_c: 1.00
[4211] loss_f: 2.31 loss_c: 0.99
[4221] loss_f: 2.26 loss_c: 0.99
[4231] loss_f: 1.80 loss_c: 1.01
[4241] loss_f: 2.18 loss_c: 0.99
[4251] loss_f: 1.72 loss_c: 1.00
[4261] loss_f: 1.86 loss_c: 1.02
[4271] loss_f: 1.62 loss_c: 0.99
[4281] loss_f: 1.78 loss_c: 1.00
[4291] loss_f: 1.66 loss_c: 1.00
[4301] loss_f: 1.62 loss_c: 1.03
[4311] loss_f: 1.93 loss_c: 1.02
[4321] loss_f: 1.65 loss_c: 0.99
[4331] loss_f: 1.86 loss_c: 1.01
[4341] loss_f: 2.06 loss_c: 0.99
[4351] loss_f: 2.75 loss_c: 1.03
[4361] loss_f: 2.24 loss_c: 1.06
[4371] loss_f: 1.90 loss_c: 0.99
[4381] loss_f: 1.83 loss_c: 0.99
[4391] loss_f: 1.66 loss_c: 1.00
[4401] loss_f: 1.73 loss_c: 0.99
[4411] loss_f: 1.83 loss_c: 1.00
[4421] loss_f: 1.66 loss_c: 0.99
[4431] loss_f: 2.09 loss_c: 0.98
[4441] loss_f: 1.79 loss_c: 1.02
[4451] loss_f: 2.23 loss_c: 1.00
[4461] loss_f: 2.44 loss_c: 1.00
[4471] loss_f: 1.79 loss_c: 1.01
[4481] loss_f: 1.63 loss_c: 0.99
[4491] loss_f: 1.86 loss_c: 1.00
[4501] loss_f: 2.14 loss_c: 0.99
[4511] loss_f: 1.61 loss_c: 1.00
[4521] loss_f: 2.08 loss_c: 1.03
[4531] loss_f: 1.74 loss_c: 1.01
[4541] loss_f: 1.93 loss_c: 0.99
[4551] loss_f: 1.88 loss_c: 1.01
[4561] loss_f: 1.65 loss_c: 1.02
[4571] loss_f: 1.74 loss_c: 1.02
[4581] loss_f: 1.55 loss_c: 1.00
[4591] loss_f: 1.77 loss_c: 0.99
[4601] loss_f: 1.55 loss_c: 1.01
[4611] loss_f: 1.65 loss_c: 1.01
[4621] loss_f: 1.77 loss_c: 0.99
[4631] loss_f: 1.62 loss_c: 1.00
[4641] loss_f: 2.04 loss_c: 1.01
[4651] loss_f: 1.96 loss_c: 1.00
[4661] loss_f: 2.17 loss_c: 0.97
[4671] loss_f: 1.84 loss_c: 0.99
[4681] loss_f: 1.56 loss_c: 0.98
[4691] loss_f: 1.82 loss_c: 0.99
[4701] loss_f: 1.71 loss_c: 0.99
[4711] loss_f: 1.72 loss_c: 1.02
[4721] loss_f: 1.56 loss_c: 0.99
[4731] loss_f: 1.68 loss_c: 1.00
[4741] loss_f: 1.65 loss_c: 0.99
[4751] loss_f: 1.54 loss_c: 0.99
[4761] loss_f: 2.87 loss_c: 1.08
[4771] loss_f: 2.29 loss_c: 1.08
[4781] loss_f: 2.71 loss_c: 1.04
[4791] loss_f: 2.26 loss_c: 1.05
[4801] loss_f: 2.47 loss_c: 1.05
[4811] loss_f: 2.04 loss_c: 1.04
[4821] loss_f: 2.07 loss_c: 1.10
[4831] loss_f: 2.05 loss_c: 1.07
[4841] loss_f: 1.86 loss_c: 1.05
[4851] loss_f: 1.87 loss_c: 1.05
[4861] loss_f: 2.06 loss_c: 1.04
[4871] loss_f: 1.66 loss_c: 1.04
[4881] loss_f: 1.82 loss_c: 1.01
[4891] loss_f: 1.84 loss_c: 1.02
[4901] loss_f: 1.98 loss_c: 1.08
[4911] loss_f: 1.89 loss_c: 1.02
[4921] loss_f: 1.80 loss_c: 1.00
[4931] loss_f: 1.84 loss_c: 1.00
[4941] loss_f: 1.77 loss_c: 1.02
[4951] loss_f: 1.91 loss_c: 0.98
[4961] loss_f: 1.82 loss_c: 0.99
[4971] loss_f: 1.77 loss_c: 0.99
[4981] loss_f: 1.55 loss_c: 0.98
[4991] loss_f: 1.83 loss_c: 0.98
[5001] loss_f: 1.61 loss_c: 0.99
[5011] loss_f: 1.84 loss_c: 1.00
[5021] loss_f: 1.86 loss_c: 0.99
[5031] loss_f: 2.01 loss_c: 0.98
[5041] loss_f: 1.76 loss_c: 0.98
[5051] loss_f: 1.95 loss_c: 0.99
[5061] loss_f: 1.71 loss_c: 1.03
[5071] loss_f: 2.00 loss_c: 0.98
[5081] loss_f: 2.41 loss_c: 1.00
[5091] loss_f: 1.79 loss_c: 0.99
[5101] loss_f: 1.86 loss_c: 0.99
[5111] loss_f: 1.69 loss_c: 1.05
[5121] loss_f: 1.88 loss_c: 1.00
[5131] loss_f: 1.61 loss_c: 1.06
[5141] loss_f: 1.71 loss_c: 1.00
[5151] loss_f: 1.80 loss_c: 0.99
[5161] loss_f: 1.87 loss_c: 1.01
[5171] loss_f: 1.60 loss_c: 0.98
[5181] loss_f: 1.66 loss_c: 0.99
[5191] loss_f: 1.75 loss_c: 0.99
[5201] loss_f: 1.77 loss_c: 1.00
[5211] loss_f: 2.14 loss_c: 1.01
[5221] loss_f: 2.14 loss_c: 1.00
[5231] loss_f: 2.09 loss_c: 0.99
[5241] loss_f: 1.59 loss_c: 1.00
[5251] loss_f: 1.80 loss_c: 1.00
[5261] loss_f: 2.59 loss_c: 0.99
[5271] loss_f: 1.92 loss_c: 0.99
[5281] loss_f: 1.78 loss_c: 1.01
[5291] loss_f: 2.19 loss_c: 0.97
[5301] loss_f: 1.67 loss_c: 1.00
[5311] loss_f: 1.67 loss_c: 1.00
[5321] loss_f: 1.76 loss_c: 1.07
[5331] loss_f: 1.76 loss_c: 1.02
[5341] loss_f: 1.50 loss_c: 0.99
[5351] loss_f: 1.75 loss_c: 0.97
[5361] loss_f: 1.71 loss_c: 0.98
[5371] loss_f: 1.60 loss_c: 0.98
[5381] loss_f: 1.63 loss_c: 0.98
[5391] loss_f: 1.85 loss_c: 0.98
[5401] loss_f: 2.23 loss_c: 0.99
[5411] loss_f: 1.53 loss_c: 0.98
[5421] loss_f: 1.46 loss_c: 0.99
[5431] loss_f: 1.84 loss_c: 0.99
[5441] loss_f: 2.08 loss_c: 1.00
[5451] loss_f: 1.47 loss_c: 0.99
[5461] loss_f: 1.68 loss_c: 1.00
[5471] loss_f: 1.50 loss_c: 0.99
[5481] loss_f: 1.65 loss_c: 1.00
[5491] loss_f: 2.15 loss_c: 1.00
[5501] loss_f: 1.75 loss_c: 1.00
[5511] loss_f: 1.72 loss_c: 1.00
[5521] loss_f: 1.83 loss_c: 1.00
[5531] loss_f: 1.72 loss_c: 1.02
[5541] loss_f: 1.69 loss_c: 1.00
[5551] loss_f: 1.83 loss_c: 1.01
[5561] loss_f: 1.55 loss_c: 0.99
[5571] loss_f: 1.42 loss_c: 0.98
[5581] loss_f: 1.57 loss_c: 1.06
[5591] loss_f: 1.48 loss_c: 1.00
[5601] loss_f: 1.65 loss_c: 1.04
[5611] loss_f: 1.69 loss_c: 1.00
[5621] loss_f: 1.44 loss_c: 0.97
[5631] loss_f: 1.67 loss_c: 0.99
[5641] loss_f: 1.48 loss_c: 0.98
[5651] loss_f: 2.06 loss_c: 0.99
[5661] loss_f: 2.48 loss_c: 1.00
[5671] loss_f: 1.64 loss_c: 1.01
[5681] loss_f: 1.71 loss_c: 1.00
[5691] loss_f: 1.71 loss_c: 1.00
[5701] loss_f: 1.77 loss_c: 1.01
[5711] loss_f: 1.84 loss_c: 1.01
[5721] loss_f: 2.03 loss_c: 1.00
[5731] loss_f: 2.19 loss_c: 1.02
[5741] loss_f: 1.98 loss_c: 1.04
[5751] loss_f: 1.63 loss_c: 1.06
[5761] loss_f: 2.28 loss_c: 1.04
[5771] loss_f: 1.74 loss_c: 1.01
[5781] loss_f: 1.83 loss_c: 1.02
[5791] loss_f: 2.04 loss_c: 1.02
[5801] loss_f: 1.73 loss_c: 1.02
[5811] loss_f: 1.75 loss_c: 1.05
[5821] loss_f: 1.45 loss_c: 1.06
[5831] loss_f: 1.58 loss_c: 1.06
[5841] loss_f: 1.48 loss_c: 1.08
[5851] loss_f: 1.66 loss_c: 1.13
[5861] loss_f: 1.59 loss_c: 1.10
[5871] loss_f: 1.49 loss_c: 1.07
[5881] loss_f: 1.52 loss_c: 1.07
[5891] loss_f: 2.02 loss_c: 1.06
[5901] loss_f: 2.13 loss_c: 1.03
[5911] loss_f: 1.85 loss_c: 1.01
[5921] loss_f: 1.71 loss_c: 1.01
[5931] loss_f: 2.03 loss_c: 1.01
[5941] loss_f: 1.71 loss_c: 1.01
[5951] loss_f: 1.80 loss_c: 1.04
[5961] loss_f: 1.79 loss_c: 1.02
[5971] loss_f: 1.44 loss_c: 1.03
[5981] loss_f: 1.42 loss_c: 1.04
[5991] loss_f: 1.81 loss_c: 1.02
[6001] loss_f: 1.81 loss_c: 1.02
[6011] loss_f: 1.88 loss_c: 1.00
[6021] loss_f: 2.60 loss_c: 1.03
[6031] loss_f: 1.74 loss_c: 1.02
[6041] loss_f: 1.70 loss_c: 1.01
[6051] loss_f: 1.94 loss_c: 1.01
[6061] loss_f: 1.75 loss_c: 1.04
[6071] loss_f: 2.15 loss_c: 1.01
[6081] loss_f: 1.61 loss_c: 1.04
[6091] loss_f: 1.66 loss_c: 1.05
[6101] loss_f: 2.27 loss_c: 1.02
[6111] loss_f: 2.43 loss_c: 1.03
[6121] loss_f: 2.37 loss_c: 1.03
[6131] loss_f: 1.61 loss_c: 1.03
[6141] loss_f: 1.88 loss_c: 1.03
[6151] loss_f: 1.84 loss_c: 1.01
[6161] loss_f: 1.61 loss_c: 1.01
[6171] loss_f: 1.72 loss_c: 1.02
[6181] loss_f: 1.70 loss_c: 1.01
[6191] loss_f: 1.65 loss_c: 1.04
[6201] loss_f: 1.52 loss_c: 1.01
[6211] loss_f: 1.62 loss_c: 1.02
[6221] loss_f: 1.56 loss_c: 1.07
[6231] loss_f: 1.52 loss_c: 1.01
[6241] loss_f: 1.69 loss_c: 1.01
[6251] loss_f: 1.70 loss_c: 1.00
[6261] loss_f: 1.51 loss_c: 1.00
[6271] loss_f: 1.75 loss_c: 1.02
[6281] loss_f: 1.79 loss_c: 1.02
[6291] loss_f: 1.59 loss_c: 1.00
[6301] loss_f: 1.60 loss_c: 1.02
[6311] loss_f: 1.50 loss_c: 1.03
[6321] loss_f: 1.66 loss_c: 1.03
[6331] loss_f: 1.45 loss_c: 1.02
[6341] loss_f: 1.58 loss_c: 1.04
[6351] loss_f: 2.18 loss_c: 1.01
[6361] loss_f: 1.64 loss_c: 1.01
[6371] loss_f: 1.48 loss_c: 1.01
[6381] loss_f: 1.65 loss_c: 1.03
[6391] loss_f: 1.43 loss_c: 1.02
[6401] loss_f: 1.43 loss_c: 1.01
[6411] loss_f: 1.60 loss_c: 1.01
[6421] loss_f: 1.49 loss_c: 1.02
[6431] loss_f: 1.39 loss_c: 1.03
[6441] loss_f: 1.42 loss_c: 1.00
[6451] loss_f: 1.53 loss_c: 1.00
[6461] loss_f: 2.05 loss_c: 1.01
[6471] loss_f: 1.75 loss_c: 1.02
[6481] loss_f: 1.74 loss_c: 1.01
[6491] loss_f: 1.70 loss_c: 1.02
[6501] loss_f: 1.52 loss_c: 1.00
[6511] loss_f: 1.63 loss_c: 1.02
[6521] loss_f: 1.48 loss_c: 1.03
[6531] loss_f: 1.71 loss_c: 1.02
[6541] loss_f: 1.55 loss_c: 1.02
[6551] loss_f: 1.26 loss_c: 1.01
[6561] loss_f: 1.87 loss_c: 1.01
[6571] loss_f: 1.98 loss_c: 1.01
[6581] loss_f: 1.70 loss_c: 1.04
[6591] loss_f: 1.88 loss_c: 1.01
[6601] loss_f: 1.73 loss_c: 0.99
[6611] loss_f: 1.40 loss_c: 1.02
[6621] loss_f: 1.66 loss_c: 1.01
[6631] loss_f: 1.42 loss_c: 1.01
[6641] loss_f: 1.50 loss_c: 1.03
[6651] loss_f: 1.48 loss_c: 1.04
[6661] loss_f: 1.50 loss_c: 1.08
[6671] loss_f: 1.43 loss_c: 1.02
[6681] loss_f: 1.63 loss_c: 0.99
[6691] loss_f: 1.50 loss_c: 1.00
[6701] loss_f: 1.53 loss_c: 1.02
[6711] loss_f: 1.94 loss_c: 1.02
[6721] loss_f: 1.44 loss_c: 1.00
[6731] loss_f: 1.58 loss_c: 1.00
[6741] loss_f: 1.79 loss_c: 1.01
[6751] loss_f: 1.75 loss_c: 0.97
[6761] loss_f: 1.66 loss_c: 1.01
[6771] loss_f: 1.80 loss_c: 1.00
[6781] loss_f: 1.61 loss_c: 1.01
[6791] loss_f: 1.56 loss_c: 1.01
[6801] loss_f: 1.68 loss_c: 1.01
[6811] loss_f: 1.51 loss_c: 1.01
[6821] loss_f: 1.62 loss_c: 0.99
[6831] loss_f: 1.81 loss_c: 1.00
[6841] loss_f: 1.66 loss_c: 1.04
[6851] loss_f: 1.71 loss_c: 1.00
[6861] loss_f: 1.59 loss_c: 1.02
[6871] loss_f: 1.56 loss_c: 1.00
[6881] loss_f: 1.61 loss_c: 1.01
[6891] loss_f: 1.55 loss_c: 1.01
[6901] loss_f: 1.48 loss_c: 1.00
[6911] loss_f: 1.82 loss_c: 1.01
[6921] loss_f: 1.43 loss_c: 1.01
[6931] loss_f: 1.56 loss_c: 1.01
[6941] loss_f: 1.61 loss_c: 0.99
[6951] loss_f: 1.62 loss_c: 0.99
[6961] loss_f: 1.30 loss_c: 1.00
[6971] loss_f: 1.69 loss_c: 1.02
[6981] loss_f: 1.33 loss_c: 1.06
[6991] loss_f: 1.69 loss_c: 1.06
[7001] loss_f: 1.58 loss_c: 1.03
[7011] loss_f: 1.87 loss_c: 1.00
[7021] loss_f: 1.64 loss_c: 1.00
[7031] loss_f: 1.52 loss_c: 1.00
[7041] loss_f: 2.37 loss_c: 1.01
[7051] loss_f: 1.58 loss_c: 1.00
[7061] loss_f: 1.99 loss_c: 1.01
[7071] loss_f: 1.59 loss_c: 1.02
[7081] loss_f: 1.42 loss_c: 1.03
[7091] loss_f: 1.57 loss_c: 1.03
[7101] loss_f: 1.65 loss_c: 1.00
[7111] loss_f: 1.72 loss_c: 1.00
[7121] loss_f: 1.66 loss_c: 1.00
[7131] loss_f: 1.93 loss_c: 1.01
[7141] loss_f: 1.60 loss_c: 1.02
[7151] loss_f: 1.57 loss_c: 1.10
[7161] loss_f: 1.45 loss_c: 1.09
[7171] loss_f: 1.42 loss_c: 1.25
[7181] loss_f: 1.67 loss_c: 1.61
[7191] loss_f: 1.59 loss_c: 1.53
[7201] loss_f: 1.45 loss_c: 1.22
[7211] loss_f: 1.53 loss_c: 1.23
[7221] loss_f: 2.00 loss_c: 1.15
[7231] loss_f: 1.59 loss_c: 1.12
[7241] loss_f: 1.72 loss_c: 1.10
[7251] loss_f: 1.67 loss_c: 1.10
[7261] loss_f: 1.55 loss_c: 1.06
[7271] loss_f: 1.45 loss_c: 1.05
[7281] loss_f: 1.66 loss_c: 1.05
[7291] loss_f: 1.58 loss_c: 1.06
[7301] loss_f: 2.78 loss_c: 1.05
[7311] loss_f: 1.85 loss_c: 1.07
[7321] loss_f: 1.67 loss_c: 1.05
[7331] loss_f: 1.65 loss_c: 1.05
[7341] loss_f: 1.74 loss_c: 1.07
[7351] loss_f: 1.53 loss_c: 1.03
[7361] loss_f: 1.51 loss_c: 1.03
[7371] loss_f: 1.98 loss_c: 1.10
[7381] loss_f: 1.83 loss_c: 1.03
[7391] loss_f: 1.44 loss_c: 1.01
[7401] loss_f: 1.58 loss_c: 1.03
[7411] loss_f: 1.75 loss_c: 1.02
[7421] loss_f: 1.65 loss_c: 1.03
[7431] loss_f: 1.54 loss_c: 1.02
[7441] loss_f: 1.50 loss_c: 1.02
[7451] loss_f: 1.57 loss_c: 1.02
[7461] loss_f: 1.44 loss_c: 1.02
[7471] loss_f: 1.48 loss_c: 1.04
[7481] loss_f: 1.66 loss_c: 1.05
[7491] loss_f: 1.50 loss_c: 1.02
[7501] loss_f: 1.97 loss_c: 1.03
[7511] loss_f: 1.67 loss_c: 1.03
[7521] loss_f: 1.60 loss_c: 1.02
[7531] loss_f: 1.67 loss_c: 1.01
[7541] loss_f: 1.54 loss_c: 1.02
[7551] loss_f: 1.73 loss_c: 1.04
[7561] loss_f: 1.54 loss_c: 1.02
[7571] loss_f: 1.52 loss_c: 1.02
[7581] loss_f: 1.65 loss_c: 1.03
[7591] loss_f: 1.45 loss_c: 1.02
[7601] loss_f: 1.85 loss_c: 1.04
[7611] loss_f: 1.56 loss_c: 1.02
[7621] loss_f: 1.57 loss_c: 1.01
[7631] loss_f: 1.51 loss_c: 1.03
[7641] loss_f: 1.82 loss_c: 1.02
[7651] loss_f: 1.49 loss_c: 1.03
[7661] loss_f: 1.42 loss_c: 1.02
[7671] loss_f: 1.55 loss_c: 1.02
[7681] loss_f: 1.34 loss_c: 1.04
[7691] loss_f: 1.46 loss_c: 1.02
[7701] loss_f: 1.50 loss_c: 1.05
[7711] loss_f: 1.30 loss_c: 1.03
[7721] loss_f: 1.40 loss_c: 1.02
[7731] loss_f: 1.48 loss_c: 1.05
[7741] loss_f: 1.80 loss_c: 1.03
[7751] loss_f: 1.35 loss_c: 1.02
[7761] loss_f: 1.56 loss_c: 1.03
[7771] loss_f: 1.90 loss_c: 1.03
[7781] loss_f: 1.51 loss_c: 1.03
[7791] loss_f: 1.72 loss_c: 1.03
[7801] loss_f: 1.44 loss_c: 1.02
[7811] loss_f: 1.56 loss_c: 1.02
[7821] loss_f: 1.61 loss_c: 1.02
[7831] loss_f: 2.02 loss_c: 1.06
[7841] loss_f: 1.58 loss_c: 1.01
[7851] loss_f: 1.82 loss_c: 1.04
[7861] loss_f: 1.55 loss_c: 1.03
[7871] loss_f: 1.88 loss_c: 1.02
[7881] loss_f: 1.76 loss_c: 1.02
[7891] loss_f: 1.72 loss_c: 1.04
[7901] loss_f: 1.98 loss_c: 1.04
[7911] loss_f: 1.65 loss_c: 1.03
[7921] loss_f: 1.59 loss_c: 1.03
[7931] loss_f: 1.81 loss_c: 1.08
[7941] loss_f: 1.58 loss_c: 1.05
[7951] loss_f: 1.44 loss_c: 1.00
[7961] loss_f: 1.57 loss_c: 1.01
[7971] loss_f: 1.54 loss_c: 1.04
[7981] loss_f: 1.25 loss_c: 1.02
[7991] loss_f: 1.62 loss_c: 1.03
[8001] loss_f: 1.42 loss_c: 1.02
[8011] loss_f: 1.68 loss_c: 1.02
[8021] loss_f: 1.78 loss_c: 1.04
[8031] loss_f: 1.50 loss_c: 1.06
[8041] loss_f: 1.60 loss_c: 1.04
[8051] loss_f: 1.53 loss_c: 1.03
[8061] loss_f: 1.60 loss_c: 1.02
[8071] loss_f: 1.39 loss_c: 1.02
[8081] loss_f: 1.52 loss_c: 1.01
[8091] loss_f: 1.45 loss_c: 1.01
[8101] loss_f: 1.48 loss_c: 1.00
[8111] loss_f: 1.82 loss_c: 1.01
[8121] loss_f: 1.79 loss_c: 1.04
[8131] loss_f: 1.55 loss_c: 1.03
[8141] loss_f: 1.59 loss_c: 1.03
[8151] loss_f: 1.37 loss_c: 1.08
[8161] loss_f: 1.57 loss_c: 1.03
[8171] loss_f: 1.31 loss_c: 1.01
[8181] loss_f: 1.53 loss_c: 1.03
[8191] loss_f: 1.48 loss_c: 1.03
[8201] loss_f: 1.53 loss_c: 1.05
[8211] loss_f: 1.39 loss_c: 1.06
[8221] loss_f: 1.51 loss_c: 1.02
[8231] loss_f: 1.44 loss_c: 1.03
[8241] loss_f: 1.53 loss_c: 1.03
[8251] loss_f: 1.54 loss_c: 1.03
[8261] loss_f: 1.50 loss_c: 1.03
[8271] loss_f: 1.91 loss_c: 1.04
[8281] loss_f: 1.56 loss_c: 1.12
[8291] loss_f: 1.73 loss_c: 1.11
[8301] loss_f: 1.76 loss_c: 1.04
[8311] loss_f: 2.01 loss_c: 1.01
[8321] loss_f: 1.81 loss_c: 1.03
[8331] loss_f: 1.47 loss_c: 1.03
[8341] loss_f: 1.66 loss_c: 1.02
[8351] loss_f: 1.59 loss_c: 1.02
[8361] loss_f: 1.55 loss_c: 1.04
[8371] loss_f: 1.60 loss_c: 1.03
[8381] loss_f: 1.47 loss_c: 1.02
[8391] loss_f: 1.81 loss_c: 1.07
[8401] loss_f: 1.86 loss_c: 1.02
[8411] loss_f: 1.67 loss_c: 1.02
[8421] loss_f: 1.63 loss_c: 1.01
[8431] loss_f: 1.81 loss_c: 1.03
[8441] loss_f: 1.75 loss_c: 1.03
[8451] loss_f: 1.53 loss_c: 1.02
[8461] loss_f: 1.39 loss_c: 1.01
[8471] loss_f: 1.28 loss_c: 1.00
[8481] loss_f: 1.36 loss_c: 1.00
[8491] loss_f: 1.83 loss_c: 1.01
[8501] loss_f: 1.69 loss_c: 1.01
[8511] loss_f: 1.47 loss_c: 1.01
[8521] loss_f: 1.40 loss_c: 1.00
[8531] loss_f: 1.45 loss_c: 1.02
[8541] loss_f: 1.38 loss_c: 1.03
[8551] loss_f: 1.87 loss_c: 1.02
[8561] loss_f: 1.56 loss_c: 1.02
[8571] loss_f: 1.57 loss_c: 1.02
[8581] loss_f: 1.52 loss_c: 1.02
[8591] loss_f: 1.26 loss_c: 1.01
[8601] loss_f: 1.48 loss_c: 1.00
[8611] loss_f: 1.49 loss_c: 1.01
[8621] loss_f: 1.33 loss_c: 0.99
[8631] loss_f: 1.50 loss_c: 1.05
[8641] loss_f: 1.37 loss_c: 1.04
[8651] loss_f: 1.87 loss_c: 1.03
[8661] loss_f: 1.41 loss_c: 1.01
[8671] loss_f: 1.44 loss_c: 1.00
[8681] loss_f: 1.44 loss_c: 1.01
[8691] loss_f: 1.33 loss_c: 0.99
[8701] loss_f: 1.44 loss_c: 1.00
[8711] loss_f: 1.41 loss_c: 1.02
[8721] loss_f: 1.39 loss_c: 1.01
[8731] loss_f: 1.24 loss_c: 1.02
[8741] loss_f: 1.92 loss_c: 1.01
[8751] loss_f: 1.41 loss_c: 1.00
[8761] loss_f: 1.39 loss_c: 1.00
[8771] loss_f: 1.53 loss_c: 1.03
[8781] loss_f: 1.62 loss_c: 1.02
[8791] loss_f: 1.55 loss_c: 1.06
[8801] loss_f: 1.70 loss_c: 1.03
[8811] loss_f: 1.46 loss_c: 0.99
[8821] loss_f: 1.31 loss_c: 1.01
[8831] loss_f: 1.66 loss_c: 1.01
[8841] loss_f: 1.31 loss_c: 1.00
[8851] loss_f: 2.22 loss_c: 0.99
[8861] loss_f: 1.48 loss_c: 1.00
[8871] loss_f: 1.43 loss_c: 1.01
[8881] loss_f: 1.29 loss_c: 1.00
[8891] loss_f: 1.56 loss_c: 1.03
[8901] loss_f: 1.57 loss_c: 1.02
[8911] loss_f: 1.40 loss_c: 1.00
[8921] loss_f: 1.76 loss_c: 1.01
[8931] loss_f: 1.48 loss_c: 1.03
[8941] loss_f: 1.99 loss_c: 0.99
[8951] loss_f: 1.72 loss_c: 1.00
[8961] loss_f: 1.56 loss_c: 1.00
[8971] loss_f: 1.53 loss_c: 0.99
[8981] loss_f: 1.36 loss_c: 1.01
[8991] loss_f: 1.42 loss_c: 1.02
[9001] loss_f: 1.58 loss_c: 0.99
[9011] loss_f: 1.54 loss_c: 0.99
[9021] loss_f: 1.53 loss_c: 1.00
[9031] loss_f: 1.65 loss_c: 1.01
[9041] loss_f: 1.77 loss_c: 1.00
[9051] loss_f: 1.83 loss_c: 1.01
[9061] loss_f: 2.15 loss_c: 1.00
[9071] loss_f: 1.42 loss_c: 1.00
[9081] loss_f: 1.42 loss_c: 1.00
[9091] loss_f: 1.30 loss_c: 0.99
[9101] loss_f: 1.44 loss_c: 1.01
[9111] loss_f: 1.32 loss_c: 1.00
[9121] loss_f: 1.46 loss_c: 1.02
[9131] loss_f: 1.45 loss_c: 0.99
[9141] loss_f: 1.41 loss_c: 1.02
[9151] loss_f: 1.24 loss_c: 0.98
[9161] loss_f: 1.62 loss_c: 0.99
[9171] loss_f: 1.48 loss_c: 1.00
[9181] loss_f: 1.66 loss_c: 0.99
[9191] loss_f: 1.59 loss_c: 1.00
[9201] loss_f: 1.63 loss_c: 0.99
[9211] loss_f: 1.35 loss_c: 0.99
[9221] loss_f: 1.48 loss_c: 0.99
[9231] loss_f: 1.74 loss_c: 0.99
[9241] loss_f: 1.72 loss_c: 0.99
[9251] loss_f: 1.45 loss_c: 0.99
[9261] loss_f: 1.64 loss_c: 0.99
[9271] loss_f: 1.64 loss_c: 1.01
[9281] loss_f: 1.45 loss_c: 0.99
[9291] loss_f: 1.50 loss_c: 1.00
[9301] loss_f: 1.44 loss_c: 0.99
[9311] loss_f: 1.31 loss_c: 1.00
[9321] loss_f: 1.41 loss_c: 0.99
[9331] loss_f: 1.43 loss_c: 0.98
[9341] loss_f: 1.57 loss_c: 1.00
[9351] loss_f: 2.21 loss_c: 1.00
[9361] loss_f: 1.33 loss_c: 0.98
[9371] loss_f: 1.37 loss_c: 1.01
[9381] loss_f: 1.64 loss_c: 1.01
[9391] loss_f: 1.44 loss_c: 0.99
[9401] loss_f: 1.53 loss_c: 1.00
[9411] loss_f: 1.58 loss_c: 1.01
[9421] loss_f: 1.53 loss_c: 0.99
[9431] loss_f: 1.58 loss_c: 0.99
[9441] loss_f: 1.38 loss_c: 1.02
[9451] loss_f: 1.61 loss_c: 1.00
[9461] loss_f: 1.59 loss_c: 1.02
[9471] loss_f: 1.58 loss_c: 1.00
[9481] loss_f: 1.37 loss_c: 1.00
[9491] loss_f: 1.61 loss_c: 1.00
[9501] loss_f: 1.75 loss_c: 1.01
[9511] loss_f: 1.50 loss_c: 0.99
[9521] loss_f: 1.62 loss_c: 1.00
[9531] loss_f: 1.72 loss_c: 0.99
[9541] loss_f: 1.42 loss_c: 0.99
[9551] loss_f: 1.75 loss_c: 0.98
[9561] loss_f: 1.40 loss_c: 1.02
[9571] loss_f: 1.24 loss_c: 0.98
[9581] loss_f: 1.30 loss_c: 1.01
[9591] loss_f: 1.42 loss_c: 1.04
[9601] loss_f: 1.42 loss_c: 1.03
[9611] loss_f: 1.51 loss_c: 1.00
Traceback (most recent call last):
  File "/home/oyoungquist/.conda/envs/rina/lib/python3.11/multiprocessing/util.py", line 303, in _run_finalizers
    finalizer()
  File "/home/oyoungquist/.conda/envs/rina/lib/python3.11/multiprocessing/util.py", line 227, in __call__
    res = self._callback(*self._args, **self._kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/oyoungquist/.conda/envs/rina/lib/python3.11/multiprocessing/util.py", line 136, in _remove_temp_dir
    rmtree(tempdir, onerror=onerror)
  File "/home/oyoungquist/.conda/envs/rina/lib/python3.11/shutil.py", line 763, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/home/oyoungquist/.conda/envs/rina/lib/python3.11/shutil.py", line 761, in rmtree
    os.rmdir(path, dir_fd=dir_fd)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-bq479y_b'
[9621] loss_f: 1.34 loss_c: 1.00
[9631] loss_f: 1.59 loss_c: 1.00
[9641] loss_f: 1.30 loss_c: 1.00
[9651] loss_f: 1.30 loss_c: 1.00
[9661] loss_f: 1.30 loss_c: 1.00
[9671] loss_f: 1.50 loss_c: 1.02
[9681] loss_f: 1.78 loss_c: 1.00
[9691] loss_f: 1.94 loss_c: 1.00
[9701] loss_f: 1.69 loss_c: 1.02
[9711] loss_f: 1.50 loss_c: 1.01
[9721] loss_f: 1.38 loss_c: 0.99
[9731] loss_f: 1.37 loss_c: 1.00
[9741] loss_f: 1.40 loss_c: 0.98
[9751] loss_f: 1.41 loss_c: 1.04
[9761] loss_f: 1.73 loss_c: 1.12
[9771] loss_f: 1.42 loss_c: 1.03
[9781] loss_f: 1.44 loss_c: 1.00
[9791] loss_f: 1.46 loss_c: 0.99
[9801] loss_f: 1.70 loss_c: 1.00
[9811] loss_f: 1.49 loss_c: 1.01
[9821] loss_f: 1.83 loss_c: 0.99
[9831] loss_f: 1.93 loss_c: 1.01
[9841] loss_f: 1.41 loss_c: 0.99
[9851] loss_f: 1.28 loss_c: 1.00
[9861] loss_f: 1.27 loss_c: 1.01
[9871] loss_f: 1.36 loss_c: 1.05
[9881] loss_f: 1.34 loss_c: 1.02
[9891] loss_f: 1.28 loss_c: 0.99
[9901] loss_f: 1.43 loss_c: 1.03
[9911] loss_f: 1.35 loss_c: 1.05
[9921] loss_f: 1.46 loss_c: 1.00
[9931] loss_f: 1.49 loss_c: 1.02
[9941] loss_f: 1.69 loss_c: 1.01
[9951] loss_f: 1.56 loss_c: 1.01
[9961] loss_f: 1.45 loss_c: 1.01
[9971] loss_f: 1.48 loss_c: 1.02
[9981] loss_f: 1.95 loss_c: 1.00
[9991] loss_f: 1.36 loss_c: 1.00
**** : 10lb ****
Before learning: MSE is  9.64
Mean predictor: MSE is  9.31
After learning phi(x): MSE is  1.47

**** : 10lb_left ****
Before learning: MSE is  9.57
Mean predictor: MSE is  9.21
After learning phi(x): MSE is  2.24

**** : 5lb_front ****
Before learning: MSE is  7.56
Mean predictor: MSE is  7.37
After learning phi(x): MSE is  1.53

**** : 10lb_front ****
Before learning: MSE is  9.66
Mean predictor: MSE is  9.30
After learning phi(x): MSE is  1.96

**** : 5lb ****
Before learning: MSE is  8.56
Mean predictor: MSE is  8.38
After learning phi(x): MSE is  1.53

**** : 5lb_left ****
Before learning: MSE is  7.39
Mean predictor: MSE is  7.20
After learning phi(x): MSE is  1.15

**** : 12_5lb ****
Before learning: MSE is  12.00
Mean predictor: MSE is  11.62
After learning phi(x): MSE is  1.78

**** : 0lb ****
Before learning: MSE is  5.15
Mean predictor: MSE is  5.09
After learning phi(x): MSE is  0.98

**** overall averages ****

Before learning - MSE asg:  8.69
Before learning - MSE stddev:  1.91
Mean predictor - MSE avg:  8.43
Mean predictor - MSE stddev:  1.81
After learning phi(x) - MSE avg:  1.58
After learning phi(x) - MSE stddev:  0.38

